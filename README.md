# Knowledge_Distillation
Platform used - Tensorflow <br>

Knowledge distillation is a process of training a smaller model using a larger model usually known as a teacher model.<br>
It has various benifits of Transfer learning as the model has lesser parameters than the original Parent (Teacher) model. <br>

Link for paper - https://arxiv.org/abs/1503.02531

The code is reffered from the official Keras documentation
